{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "amino-shannon",
   "metadata": {},
   "source": [
    "# Treasure Hunting with Reinforcement Learning\n",
    "\n",
    "For this project, I will be implementing the Reinforcement Q-learning algorithm to help the hunter go through a maze of walls and bandits to find the optimal path to the treasure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "parliamentary-blood",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\") "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "excessive-lindsay",
   "metadata": {},
   "source": [
    "## Creating the Map Environment\n",
    "\n",
    "Imagine the 2D array below to be a 4x4 Grid Map. Therefore, if we start at Map[0][0] and go down one step, we will end up at Map[1][0]. Available moves are up, down, left, and right. The Hunter starts at [0][0] and the Treasure is located at [3][3].\n",
    "\n",
    "* The ' ' in the map represents a clear path. \n",
    "* The 'H' in the map represents the Hunter. \n",
    "* The 'W' in the map represents the Walls. \n",
    "* The 'B' in the map represents the Bandits. \n",
    "* The 'T' in the map represents the Treasure.\n",
    "\n",
    "The states are represent by a single integer from 0 - 15. Therefore, Map[0][0] represents State 0, Map[0][1] represent State 1, ... , Map[3][2] represents State 14, Map[3][3] represents State 15."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "legendary-squad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['H', 'W', 'W', ' ']\n",
      "[' ', ' ', 'W', ' ']\n",
      "['W', ' ', ' ', 'B']\n",
      "[' ', 'B', ' ', 'T']\n"
     ]
    }
   ],
   "source": [
    "Map_init = [[' '] * 4 for _ in range(4)]\n",
    "Map_init[0][0] = 'H'\n",
    "Map_init[3][3] = 'T'\n",
    "Map_init[0][1], Map_init[0][2], Map_init[1][2], Map_init[2][0] = 'W', 'W', 'W', 'W'\n",
    "Map_init[2][3], Map_init[3][1] = 'B', 'B'\n",
    "\n",
    "def printMap(Map):\n",
    "    for row in Map:\n",
    "        print(row)\n",
    "        \n",
    "printMap(Map_init)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "subject-humidity",
   "metadata": {},
   "source": [
    "### Initial Q-matrix\n",
    "\n",
    "We will be initializing the q-matrix with all zeroes. The q-matrix is of size 5X16. The reason why it has 5 columns is because it represents the 5 action Up, Down, Left, Right, Stay. The reason why it has 16 rows is because there are 16 states in the map."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "comparative-toddler",
   "metadata": {},
   "outputs": [],
   "source": [
    "q_initial = np.zeros((16,5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "polish-namibia",
   "metadata": {},
   "source": [
    "### Reward Table\n",
    "\n",
    "The reward for the map are as it follows:\n",
    "\n",
    "* Going into an empty space = 1\n",
    "* Wall obstacle = -5\n",
    "* Bandits = -10\n",
    "* Invalid move = -3\n",
    "* Treasure = 10\n",
    "\n",
    "With this reward system, I believe the Hunter will have motivation to get to the treasure all while avoiding Wall obstacles and Bandits.\n",
    "\n",
    "The table will also be 5x16 for the same reason as the Q-matrix. Column 0 represents Up, 1 represents Down, 2 represents Left, 3 represents Right, and 4 represents Stay. Column: [Up, Down, Left, Right, Stay]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "seasonal-sacramento",
   "metadata": {},
   "outputs": [],
   "source": [
    "reward_table_1 = np.array([[-3, 1, -3, -5, 1],\n",
    "                   [-3, 1, 1, -5, -5],\n",
    "                   [-3, -5, -5, 1, -5],\n",
    "                   [-3, 1, -5, -3, 1],\n",
    "                   [1, -5, -3, -1, 1],\n",
    "                   [-5, 1, 1, -5, 1],\n",
    "                   [-5, 1, 1, 1, -5],\n",
    "                   [1, -10, -5, -3, 1],\n",
    "                   [1, 1, -3, 1, -5],\n",
    "                   [1, -10, -5, 1, 1],\n",
    "                   [-5, 1, 1, -10, 1],\n",
    "                   [1, 10, 1, -3, -10],\n",
    "                   [-5, -3, -3, -10, 1],\n",
    "                   [1, -3, 1, 1, 1],\n",
    "                   [1, -3, -10, 10, 1],\n",
    "                   [-10, -3, 1, -3, 10],\n",
    "                  ])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "anticipated-method",
   "metadata": {},
   "source": [
    "### Transition Matrix\n",
    "\n",
    "This matrix will also be 5x16. The Columns also represent the same things as the Reward Matrix's column. Therefore,  Column 0 represents Up, 1 represents Down, 2 represents Left, 3 represents Right, and 4 represents Stay. Column: [Up, Down, Left, Right, Stay]\n",
    "\n",
    "The numbers in each of the matrix represent which state it transitions to. Therefore the first array in the the 2D array represents the State 0. And the number in the array represents which state it transitions to if taken the appropriate move. As an example the first array [-1, 4, -1, 1, 0] means that if you go down (2nd column), you will end up in State 4. And if you go right (4th column), you will end up in State 1. Note, that if you go up (1st column) or left (3rd column), it brings you to State -1. This mean that it is an invalid State."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "minute-insulin",
   "metadata": {},
   "outputs": [],
   "source": [
    "transition_matrix = np.array([[-1, 4, -1, 1, 0],\n",
    "                       [-1, 5, 0, 2, 1],\n",
    "                       [-1, 6, 1, 3, 2],\n",
    "                       [-1, 7, 2, -1, 3],\n",
    "                       [0, 8, -1, 5, 4],\n",
    "                       [1, 9, 4, 6, 5],\n",
    "                       [2, 10, 5, 7, 6],\n",
    "                       [3, 11, 6, -1, 7],\n",
    "                       [4, 12, -1, 9, 8],\n",
    "                       [5, 13, 8, 10, 9],\n",
    "                       [6, 14, 9, 11, 10],\n",
    "                       [7, 15, 9, -1, 11],\n",
    "                       [8, -1, -1, 13, 12],\n",
    "                       [9, -1, 12, 14, 13],\n",
    "                       [10, -1, 13, 15, 14],\n",
    "                       [11, -1, 14, -1, 15],\n",
    "                      ])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "serious-symphony",
   "metadata": {},
   "source": [
    "### Action Table\n",
    "\n",
    "The action table contains information regarding which valid action a particular state can take. We will be representing the number 0 as up, 1 as down, 2 as left, 3 as right, and 4 as stay."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "average-strike",
   "metadata": {},
   "outputs": [],
   "source": [
    "action_table = np.array([[1, 3, 4],\n",
    "                       [1, 2, 3, 4],\n",
    "                       [1, 2, 3, 4],\n",
    "                       [1, 2, 4],\n",
    "                       [0, 1, 3, 4],\n",
    "                       [0, 1, 2, 3, 4],\n",
    "                       [0, 1, 2, 3, 4],\n",
    "                       [0, 1, 2, 4],\n",
    "                       [0, 1, 3, 4],\n",
    "                       [0, 1, 2, 3, 4],\n",
    "                       [0, 1, 2, 3, 4],\n",
    "                       [0, 1, 2, 4],\n",
    "                       [0, 3, 4],\n",
    "                       [0, 2, 3, 4],\n",
    "                       [0, 2, 3, 4],\n",
    "                       [0, 2, 4]\n",
    "                      ])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fitted-knight",
   "metadata": {},
   "source": [
    "## Algorithm Implementation\n",
    "\n",
    "The algorithm allows the Hunter to travel states according to the appropriate action and update the Q-matrix using the Bellman Equation. You can specify how many iterations you would like it to run and the gamma learning rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "hundred-sensitivity",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Q_learning(gamma_rate, num_iter, q_matrix, reward_table, transition_matrix, action_table):\n",
    "    for i in range(num_iter):\n",
    "        start = 0\n",
    "        current = start\n",
    "        while current != 15:\n",
    "            action = random.choice(action_table[current])\n",
    "            next_state = transition_matrix[current][action]\n",
    "            future_reward = []\n",
    "            for action_next in action_table[next_state]:\n",
    "                future_reward.append(q_matrix[next_state][action_next])\n",
    "            q_state = reward_table[current][action] + gamma_rate*max(future_reward)\n",
    "            q_matrix[current][action] = q_state\n",
    "            current = next_state\n",
    "    return q_matrix\n",
    "\n",
    "def getOptimalDirections(q_matrix):\n",
    "    q_df = pd.DataFrame(q_matrix)\n",
    "    print(q_df)\n",
    "    optimalDirections = []\n",
    "    state = 0\n",
    "    while state != 15:\n",
    "        row = q_df.iloc[state]\n",
    "        direction = row.idxmax(axis=1)\n",
    "        state = transition_matrix[state][direction]\n",
    "        if direction == 0:\n",
    "            optimalDirections.append(\"Up\")\n",
    "        elif direction == 1:\n",
    "            optimalDirections.append(\"Down\")\n",
    "        elif direction == 2:\n",
    "            optimalDirections.append(\"Left\")\n",
    "        elif direction == 3:\n",
    "            optimalDirections.append(\"Right\")\n",
    "        else:\n",
    "            optimalDirections.append(\"Stay\")\n",
    "    return optimalDirections"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "advised-angel",
   "metadata": {},
   "source": [
    "### Running the Algorithm for the Map above\n",
    "\n",
    "Now we will run the Q-learning algorithm for the map environment above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "stretch-broadway",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          0        1        2        3        4\n",
      "0   0.00000   5.0384  0.00000   0.6384  5.03072\n",
      "1   0.00000   7.0480  5.03072  -1.0000  0.63840\n",
      "2   0.00000   1.5600  0.63840   5.0000 -1.00000\n",
      "3   0.00000   5.0000 -1.00000   0.0000  5.00000\n",
      "4   5.03072   1.0480  0.00000   5.0480  5.03840\n",
      "5   0.63840   7.5600  5.03840   1.5600  7.04800\n",
      "6  -1.00000   8.2000  7.04800   5.0000  1.56000\n",
      "7   5.00000  -2.0000  1.56000   0.0000  5.00000\n",
      "8   5.03840   5.0000  0.00000   7.5600  1.04800\n",
      "9   7.04800  -2.8000  1.04800   8.2000  7.56000\n",
      "10  1.56000   9.0000  7.56000  -2.0000  8.20000\n",
      "11  5.00000  10.0000  7.56000   0.0000 -2.00000\n",
      "12  1.04800   0.0000  0.00000  -2.8000  5.00000\n",
      "13  7.56000   0.0000  5.00000   9.0000  8.20000\n",
      "14  8.20000   0.0000 -2.80000  10.0000  9.00000\n",
      "15  0.00000   0.0000  0.00000   0.0000  0.00000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['Down', 'Right', 'Down', 'Right', 'Down', 'Right']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q_matrix = Q_learning(0.8, 100, q_initial, reward_table_1, transition_matrix, action_table)\n",
    "optimal_directions = getOptimalDirections(q_matrix)\n",
    "optimal_directions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "blind-hopkins",
   "metadata": {},
   "source": [
    "### Revisiting the Map\n",
    "\n",
    "Let's see if the optimal direction gets the Hunter to the treasure. The '*' on the map will represent the steps the Hunter took, according to the output above which is Down, then Right, then Down, then Right, then Down, and finally Right."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "transparent-reply",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['H', 'W', 'W', ' ']\n",
      "['*', '*', 'W', ' ']\n",
      "['W', '*', '*', 'B']\n",
      "[' ', 'B', '*', 'T']\n"
     ]
    }
   ],
   "source": [
    "Map_1 = Map_init\n",
    "Map_1[1][0], Map_1[1][1], Map_1[2][1], Map_1[2][2], Map_1[3][2] = '*', '*', '*', '*', '*'\n",
    "printMap(Map_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "supposed-parade",
   "metadata": {},
   "source": [
    "Looks like our algorithm works well!\n",
    "\n",
    "## New policy and reward system\n",
    "\n",
    "Now lets say we create a new Map environment. This time the Walls are made of delicious and nutritious cookies and the hunter is able to visit the wall to eat and gain a reward of +3. And now the treasure is 10x more valuable. Therefore, bandits will also be 10x worse. Will this reward system create a new policy? Lets find out!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "likely-chapter",
   "metadata": {},
   "outputs": [],
   "source": [
    "reward_table_2 = np.array([[-3, 1, -3, 2, 1],\n",
    "                   [-3, 1, 1, 2, 2],\n",
    "                   [-3, 2, 2, 1, 2],\n",
    "                   [-3, 1, 2, -3, 1],\n",
    "                   [1, 2, -3, -1, 1],\n",
    "                   [2, 1, 1, 2, 1],\n",
    "                   [2, 1, 1, 1, 2],\n",
    "                   [1, -100, 2, -3, 1],\n",
    "                   [1, 1, -3, 1, 2],\n",
    "                   [1, -100, 2, 1, 1],\n",
    "                   [2, 1, 1, -100, 1],\n",
    "                   [1, 10, 1, -3, -100],\n",
    "                   [2, -3, -3, -100, 1],\n",
    "                   [1, -3, 1, 1, 1],\n",
    "                   [1, -3, -100, 100, 1],\n",
    "                   [-100, -3, 1, -3, 100],\n",
    "                  ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fluid-vegetable",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           0        1         2         3         4\n",
      "0    0.00000  36.9296   0.00000   38.5696  31.85568\n",
      "1    0.00000  44.7120  31.85568   45.7120  38.56960\n",
      "2    0.00000  54.6400  38.56960   37.5696  45.71200\n",
      "3    0.00000  44.7120  45.71200    0.0000  37.56960\n",
      "4   31.85568  44.9120   0.00000   42.7120  36.92960\n",
      "5   38.56960  53.6400  36.92960   54.6400  44.71200\n",
      "6   45.71200  65.8000  44.71200   44.7120  54.64000\n",
      "7   37.56960 -57.0880  54.64000    0.0000  44.71200\n",
      "8   36.92960  36.9296   0.00000   53.6400  44.91200\n",
      "9   44.71200 -35.2000  44.91200   65.8000  53.64000\n",
      "10  54.64000  81.0000  53.64000  -57.0880  65.80000\n",
      "11  44.71200  10.0000  53.64000    0.0000 -57.08800\n",
      "12  44.91200   0.0000   0.00000  -35.2000  36.92960\n",
      "13  53.64000   0.0000  36.92960   81.0000  65.80000\n",
      "14  65.80000   0.0000 -35.20000  100.0000  81.00000\n",
      "15   0.00000   0.0000   0.00000    0.0000   0.00000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['Right', 'Right', 'Down', 'Down', 'Down', 'Right']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q_matrix = Q_learning(0.8, 100, q_initial, reward_table_2, transition_matrix, action_table)\n",
    "optimal_directions = getOptimalDirections(q_matrix)\n",
    "optimal_directions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "patient-mouth",
   "metadata": {},
   "source": [
    "It looks like there is a new policy! This policy does make sense as it now travels through the walls and eats them as they are made of cookies before making its way to the final treasure while avoiding the Bandits.\n",
    "\n",
    "## Conclusion\n",
    "\n",
    "Overall, I am satisfied with the results and policy created using the Reinforcement Q-learning algorithm created to help the Hunter get to his Treasure. An improvement to my implementation could be automating the way the Reward Table, Action Table, and Transition Matrix is constructed. I have learned a lot about Reinforcement Learning.\n",
    "\n",
    "## References\n",
    "1. https://towardsdatascience.com/a-beginners-guide-to-q-learning-c3e2a30a653c\n",
    "2. https://blog.floydhub.com/an-introduction-to-q-learning-reinforcement-learning/\n",
    "3. https://towardsdatascience.com/introduction-to-q-learning-88d1c4f2b49c\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
